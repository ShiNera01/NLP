언어 모델이란 단어 시퀀스(문장)에 확률을 할당하는 모델.

1) 언어 모델
  - 언어 모델은 언어라는 현상을 모델링하고자 단어 시퀀스(또는 문장)에 확률을 할당 하는모델.
  - 통계를 이용한 방법, 인공 신경망을 이용한 방법
  
  1. 언어 모델(Language Model)
    - 언어 모델은 단어 시퀀스에 확률을 할당 하는 일을 하는 모델. 가장 자연스러운 단어 시퀀스를 찾아 내는 것. 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측
    - 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델
    - 언어 모델에 -ing를 붙인 언어 모델링은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업


  2. 단어 시퀀스의 확률 할당
    - 기계 번역(Machine Translation)
      P(나는 버스를 탔다) > P(나는 버스를 태운다)
      언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단한다.
    - 오타 교정(Spell Correction)
      선생님이 교실로 부리나케
      P(달려갔다) > P(잘려갔다)
      언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단
    - 음성 인식(speech Recognition)
      P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)
      언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단
    
  3. 주어진 이전 단어들로부터 다음 단어 예측
    - 단어 시퀀스의 확률
      하나의 단어를 w, 단어 시퀀스를 대문자 W라고 한다면, n개의 단어가 등장하는 단어 시퀀스 W의 확률은
      P(W) = p(w1,w2,w3,w4,...,wn)
    - 다음 단어 등장의 확률
      P(wn|w1,w...,wn-1) | 조건부 확률(conditional probability)
      
 
 2) 통계적 언어 모델(Statistical Language Model, SLM)
  1. 조건부 확률
    - 조건부 확률은 두 확률 P(A), P(B)에 대해 아래와 같은 관계 가진다.
    
      P(B|A) = P(A,B)/P(A)
      P(A,B) = P(A)P(B|A)
      
      P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)
      
      P(x1,x2,x3,....,xn) = P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1...xn-1)
      
  2. 문장에 대한 확률 
      ![image](https://user-images.githubusercontent.com/37740450/120448841-e94cd680-c3c6-11eb-94c7-3bdbe4c5cf9d.png)

  3. 카운트 기반의 접근
  
      ![image](https://user-images.githubusercontent.com/37740450/120448986-100b0d00-c3c7-11eb-8168-3eb9bef3f12b.png)

      
  4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)
    - 언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링.
    - P(is|An adorable little boy)를 구하는 경우에서 기계가 훈련한 코퍼스에 An adorable little boy가 나왔을 때 is가 나올 확률이라는 것이 존재. 이를 실제 자연어의        확률 분포, 현실에서의 확률 분포라고 명칭.       
    - 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것은 정확하지 않은 모델
    - 충분히 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 희소 문제(sparsity problem)
      
3) N-gram 언어 모델
  - n-gram 언어 모델은 카운트에 기반한 통계적 접근을 사용하는 SLM. 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법 사용. 일부 단어를 몇    개 보내느냐를 결정하냐에 따라서 n이 달라짐

  1. 코퍼스에서 카운트하지 못하는 경우의 감소
    - SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점. 계산하고 싶은 문장이 길어질수록 갖고 있는 코퍼스에서 그 문장이 존재하지 않을       가능성이 높다. 카운트할 수 없을 가능성이 높다. 하지만 참고하는 단어들을 줄이면 카운트를 할 수 있을 가능성을 높일 수 있다.
    예를 들어, An adorable little boy가 나왔을 때 is가 나올 확률을 그냥 boy가 나왔을 때 is가 나올확률로 생각해본다면 boy is 라는 더 짧은 단어 시퀀스가 존재할 가능성
    이 더 높다. 너무 짧다면 little boy가 나왔을때 is가 나올 확률로 생각하는 것 역시도 방법이 될 수 있다.
    앞에서는 An adorable little boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 An adorable little boy가 나온 횟수와 An adorable little boy is가 나온 횟수를 카     운트 해야 하지만, 여기서는 구하고자 하는 기준 단어의 앞 단어를 전부 포함해서 카운트 하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사.
    이렇게 도 ㅣ면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률을 높일 수 있다.
    
  2. N-gram
    - 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram
    - unigrams : an, adorable, little, boy, is, spreading, smiles
      bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles
      trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles
      4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles
    
    - n 그램의 ㄷ
  
  

















