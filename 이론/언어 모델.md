언어 모델이란 단어 시퀀스(문장)에 확률을 할당하는 모델.

1) 언어 모델
  - 언어 모델은 언어라는 현상을 모델링하고자 단어 시퀀스(또는 문장)에 확률을 할당 하는모델.
  - 통계를 이용한 방법, 인공 신경망을 이용한 방법
  
  1. 언어 모델(Language Model)
    - 언어 모델은 단어 시퀀스에 확률을 할당 하는 일을 하는 모델. 가장 자연스러운 단어 시퀀스를 찾아 내는 것. 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측
    - 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델
    - 언어 모델에 -ing를 붙인 언어 모델링은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업


  2. 단어 시퀀스의 확률 할당
    - 기계 번역(Machine Translation)
      P(나는 버스를 탔다) > P(나는 버스를 태운다)
      언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단한다.
    - 오타 교정(Spell Correction)
      선생님이 교실로 부리나케
      P(달려갔다) > P(잘려갔다)
      언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단
    - 음성 인식(speech Recognition)
      P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)
      언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단
    
  3. 주어진 이전 단어들로부터 다음 단어 예측
    - 단어 시퀀스의 확률
      하나의 단어를 w, 단어 시퀀스를 대문자 W라고 한다면, n개의 단어가 등장하는 단어 시퀀스 W의 확률은
      P(W) = p(w1,w2,w3,w4,...,wn)
    - 다음 단어 등장의 확률
      P(wn|w1,w...,wn-1) | 조건부 확률(conditional probability)
      
 
 2) 통계적 언어 모델(Statistical Language Model, SLM)
  1. 조건부 확률
    - 조건부 확률은 두 확률 P(A), P(B)에 대해 아래와 같은 관계 가진다.
    
      P(B|A) = P(A,B)/P(A)
      P(A,B) = P(A)P(B|A)
      
      P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)
      
      P(x1,x2,x3,....,xn) = P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1...xn-1)
      
  2. 문장에 대한 확률 
      ![image](https://user-images.githubusercontent.com/37740450/120448841-e94cd680-c3c6-11eb-94c7-3bdbe4c5cf9d.png)

  3. 카운트 기반의 접근
  
      ![image](https://user-images.githubusercontent.com/37740450/120448986-100b0d00-c3c7-11eb-8168-3eb9bef3f12b.png)

      
  4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)
    - 언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링.
    - P(is|An adorable little boy)를 구하는 경우에서 기계가 훈련한 코퍼스에 An adorable little boy가 나왔을 때 is가 나올 확률이라는 것이 존재. 이를 실제 자연어의        확률 분포, 현실에서의 확률 분포라고 명칭.       
    - 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것은 정확하지 않은 모델
    - 충분히 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 희소 문제(sparsity problem)
      
3) N-gram 언어 모델
  - n-gram 언어 모델은 카운트에 기반한 통계적 접근을 사용하는 SLM. 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법 사용. 일부 단어를 몇    개 보내느냐를 결정하냐에 따라서 n이 달라짐

  1. 코퍼스에서 카운트하지 못하는 경우의 감소
    - SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점. 계산하고 싶은 문장이 길어질수록 갖고 있는 코퍼스에서 그 문장이 존재하지 않을       가능성이 높다. 카운트할 수 없을 가능성이 높다. 하지만 참고하는 단어들을 줄이면 카운트를 할 수 있을 가능성을 높일 수 있다.
    예를 들어, An adorable little boy가 나왔을 때 is가 나올 확률을 그냥 boy가 나왔을 때 is가 나올확률로 생각해본다면 boy is 라는 더 짧은 단어 시퀀스가 존재할 가능성
    이 더 높다. 너무 짧다면 little boy가 나왔을때 is가 나올 확률로 생각하는 것 역시도 방법이 될 수 있다.
    앞에서는 An adorable little boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 An adorable little boy가 나온 횟수와 An adorable little boy is가 나온 횟수를 카     운트 해야 하지만, 여기서는 구하고자 하는 기준 단어의 앞 단어를 전부 포함해서 카운트 하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사.
    이렇게 도 ㅣ면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률을 높일 수 있다.
    
  2. N-gram
    - 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram
    - unigrams : an, adorable, little, boy, is, spreading, smiles
      bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles
      trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles
      4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles
    - n-gram을 통한 언어 모델에서 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존
      'An adorable little boy is spreading'다음에 나올 단어를 예측하고 싶을 때 n = 4라면 boy is spreading만 고려
    - 갖고 있는 코퍼스에서 boy is spreading가 1000번 등장했다고 하고 boy is preading insults가 500번, boy is spreading smiles가 200번 등장했다고 하면 전자는 50%
      후자는 20% 확률을 가지며, insults가 더 맞다고 판단하게 됨.
    
  3. N-gram Language model 한계
    - n-gram은 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생김
     (1) 희소 문제(Sparsity Problem)
        - 문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한
        희소 문제 존재
     (2) n을 선택하는 것은 trade-off 문제
        - 몇 개의 단어를 볼 것인지 n을 정하는 것은 trade-off가 존재한다. n을 1보다 2로 선택하는 것은 대부분 경우에서 언어 모델의 성능을 높일 수 있다. 하지만 n을 크게 선택하면 실제 훈련 코퍼스
        에서 해당 n-gram을 카운트할 수 있는 확률은 적어진다. 또한 n이 커지면 모델 사이즈가 커지는 문제 역시 발생
        - n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률 분포와 멀어진다. 그렇기에 적절한 n을 선택해야 한다. trade-off 문제로 인해 정확도를 높이려면 n은 최
        대 5를 넘게 잡아서는 안된다고 권장된다.
   
   4. Domain에 맞는 코퍼스의 수집
    - 어떤 분야인지 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 전부 다르다. 마케팅 분야에서는 마케팅 단어가 빈번하게 등장할 것이고, 의료 분야에서는 의료 관련 단어가 빈번하게 등장
      언어 모델에 사용하는 코퍼스를 해당 도메인에 맞게 적용시키면 적합한 언어 생성을 할 가능성이 높아진다.
      
   5. 인공신경망 언어 모델 (Neural Network Based Language Model)
    -  N-gram Language Model 한계점을 극복하기 위해 분모, 분자에 숫자를 더해서 카운트 했을 때 0 이 되는 것을 방지하는 등의 일반화 방법들이 존재. 하지만 n-gram 언어 모델에 대한 취약점을 완전
      히 해결하지 모샜고, 이를 인공 신경망 모델로 해결
      
      
4) 한국어 언어 모델
  1. 한국어는 어순이 중요하지 않다. 어순이 달라도 의미를 파악할 수 있다.
  
  2. 한국어는 교착어다. 띄어쓰기 단위인 어절 단위로 토큰화를 할 경우에는 문장에서 발생가능한 단어의 수가 굉장히 많이 늘어난다. 한국어는 영어와 다르게 조사가 존재. '그녀'라는 단어 하나만 해도
     그녀가, 그녀를, 그녀의, 그녀와 등등의 다양한 경우가 존재.
  
  3. 한국어는 띄어쓰기가 제대로 지켜지지 않는다. 토큰이 제대로 분리되지 않은채 훈련 데이터로 사용되면 언어 모델은 제대로 동작하지 않는다.



5) Perplexity
  
  1. 언어 모델 평가 방법 PPL
    - '헷갈리는 정도'로 이해하면 된다. 수치가 낮을수록 모델의 성능이 좋은 것을 의미
    -  단어의 수로 정규화된 테스트 데이터에 대한 확률의 역수. PPL을 최소화한다는 것은 문장의 확률을 최대화 하는 것과 같다. 
  
  2. 분기 계수 (Branching factor)
    - PPL은 이 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하는지 의미한다. PPL이 10이 나왔다는 것은 해당 언어 모델이 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점
      마다 평균적으로 10개의 단어를 가지고 어떤 것이 정답인지 고민하는 것을 의미
      
 
  












