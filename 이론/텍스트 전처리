- 용도에 맞게 텍스트를 사전에 처리하는 작업


1) 토큰화 (Tokenization)
  - 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 부른다. 
  
  
  1. 단어 토큰화 (word tokenization)
    - 토큰의 기준을 단어(word)로 하는 경우, 단어구, 의미를 갖는 문자열로 간주하기도 함
    
    구두점(punctuation)과 같은 문자 제외 시키는 간단한 토큰화 작업
    input : Time is an illusion. Lunchtime!
    output : "Time", "is", "an", "illusion", "Lunchtime"
    
    해당 입력값에 구두점을 지운 뒤 띄어쓰기 기준으로 잘라냄.
    
    토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(cleaning) 작업 수행만으로 해결 불가능. 전부 제거 하는 경우 토큰의 의미를 잃어 버리는 경우 발생.
    
  2. from nltk.tokenize import word_tokenize
    - word_tokenize는 어퍼스트로피 Don`t  - > "Do", "n't"로 분리  Jone`s -> "Jone", "'s" 로 분리
     from nltk.tokenize import WordPunctTokenizer
    - wordPunctTokenizerDon`t  -> "Don", "'","t"로 분리   Jone's -> "Jone", "'", "s"로 분리
    
    케라스 토큰화
    - from tensorflow.keras.preprocessing.text import text_to_word_sequence
    모든 알파벳을 소문자로 바꾸고 마침표나 콤마, 느낌표 등의 구두점 모두 제거 don`t나 jone`s와 같은 어퍼스트로피는 보존
    
    
  3. 고려사항
    - 구두점이나 특수 문자를 단순 제외해서는 안된다.
    - 줄임말과 단어 내의 띄어쓰기가 있는 경우
    - 표준 토큰화 예제
       rule_1 : 하이푼으로 구성된 단어는 하나로 유지
       rule_2 : doesn't와 같이 어퍼스트로피 '접어'가 함께하는 단어는 분리해준다.
       
       from nltk.tokenize import TreebankWordTokenizer
       tokenizer = TreebankWordTokenizer()
       
       home-based는 하나의 토큰으로 취급 doesn`t의 경우 does와 n`t는 분리되었다.
       
       
   4. 문장 토큰화(Sequence Tokenization)
    - 코퍼스 내에서 문장 단위로 구분하는 작업으로 문장 분류 (sentence segmentation)라고도 함.
    - ?나 !는 문장의 명확한 구분자 역할 하지만 마침표(.)는 끝이 아니더라도 등장할 가능성 존재
      ex) 192.168...과 같은 ip주소  ph.D 등
      
      
      from nltk.tokenize import sent_tokenize
      text = " "
      print(sent_tokenize(text))
      
      
   5. 이진 분류기 (Binary Classifier)
     - 문장 토큰화에서의 예외 사항을 발생시키는 마침표 처리를 위해서 입력에 따라 두 개의 클래스로 분류하는 이진 분류기 사용
     
      1) 마침표가 단어의 일부분일 경우.
      2) 마침표가 문장의 구분자로 사용될 경우
      
      
      
      
      
      
2) 정제(Cleaning)와 정규화(Normalization)
    정제 : 갖고 있는 코퍼스로부터 노이즈 데이터 제거
    정규화 : 표현 방법이 다른 단어들을 통합시켜 같은 단어로 만드는 것
    
   1. 규칙에 기반한 표기가 다른 단어들의 통합
    - USA와 US는 같은 의미
    
   2. 대,소문자 통합
    - 대부분의 글은 소문자로 작성되어서, 대문자를 소문자로 변환하는 작업
    - 대,소문자가 구분되어야 하는 경우도 존재 US(미국), us(우리) 구분 되어야함. 회사 이름, 사람 이름등은 대문자로 유지되어야 한다.
  
   3. 불필요한 단어 제거(Removing Unncessary Words)
    - 정제 작업에서는 아무 의미도 갖지 않는 글자들 뿐만 아니라, 분석하고자 하는 목적에 맞지 않는 단어들 역시도 노이즈 데이터라고 정의한다.
    - 불필요 단어들을 제거하는 방법으로는 불용어 제거와 등장 빈도가 적은 단어, 짧은 단어들을 제거하는 방법
    
      1)등장 빈도가 적은 단어 (Removing Rare words)
      2)길이가 짧은 단어 (Removing words with very a short length)
        - 길이가 2,3인 단어들을 제거함으로써 의미 없는 단어를 줄이는 효과
          ex) import re
              text = ""
              shortword = re.compiler(r'|w*|b|w{1,2}|b')
              print(shortword.sub('',text))
              
    4. 정규 표현식(Regular Expression)
    
    
    
    
    
    
 3) 어간 추출(Stemming) and 표제어 추출(Lemmatization)
    
    1. 표제어 추출
      - 단어들로부터 표제어를 찾아가는 것
      - am, are, is는 뿌리 단어인 be에서 비롯되고 표제어는 be
      
      - 의미를 가진 최소의 단위  형태소는 두가지 종류
        1) 어간 (stem) : 단어의 의미를 담고 있는 단어의 핵심
        2) 접사 (affix) : 단어의 추가적인 의미
      - 형태학적인 파싱은 이 두가지 구성 요소를 분리 하는 작업
      
      ex) import nltk
      n = nltk.stem.WordNetLemmatizer()
      words = ["hi","hello","love","lives","animal","is"]
      print([n.lemmatize(i) for i in words])
      
      ['hi', 'hello', 'love', 'life', 'animal', 'is']
        
      - WordNetLemmatizer는 입력으로 단어가 동사 품사라는 것을 알려줄 수 있다.
      ex) n.lemmatize('dies', 'v')
        -> 'die'
          n.lemmatize('watched', 'v')
         -> 'watch'
         
    2. 어간 추출
      - 어간을 추출하는 작업
        ex) from nltk.stem import PorterStemmer
            from nltk.tokenize import word_tokenize
            s = PorterStemmer()
            
            text = " "
            words = word_tokenize(text)
            print(words)
            print([s.stem(i) for i in words])
            
            포터 알고리즘에서 어간 추출에 대한 규정해놓은 규칙대로 어간이 추출된다.
            
        
  4) 불용어(Stopword)
    - 자주 등장하지만 분석에 있어서 큰 도움이 되지 않는 것
      ex) I,my,me,over, 조사, 접미사 등 = stopword 
    - nltk에서 백여개 이상의 영어 단어들을 패키지로 제공
    
    
    1. NLTK
      from nltk.corpus import stopwords
      stopwords.words('english')  #nltk에서 정의 해놓은 영어 불용어 리스트 반환
      
      
    2. NLTK 이용해 불용어 제거
      from nltk.corpus import stopwords
      from nltk.tokenize import word_tokenize
      
      text = "Family is not an important thing. It`s everything."
      stop_words = set(stopwords.words('english'))
      word_tokens = word_tokenize(text)
      
      result = []
      for i in word_tokens:
        if i not in stop_words:
          result.append(i)
          
    3. 한국어 불용어 제거
      - 직접 불용어 정의하여 제거
      from nltk.corpus import stopwords
      from nltk.tokenize import word_tokenize
      
      text = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
      stop_words = "아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든"
      
      stop_words = stop_words.split(' ')
      word_tokens = word_tokenize(text)
      
      result = []
      for i in word_tokens:
        if i not in stop_words:
          result.append(i)
  5) 정규 표현식 (Regular Expression)
    
    1. 정규 표현식 문법과 모듈 함수
      . : 한 개의 임의의 문자를 나타낸다.
      ? : 앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있다. (문자 0 or 1개)
      * : 앞의 문자가 무한개로 존재할 수도 있고, 존재 하지 않을 수도 있다. (문자 0개 이상)
      + : 앞의 문자가 최소 한 개 이상
      ^ : 뒤의 문자로 문자열이 시작된다
      $ : 앞의 문자로 문자열이 끝남
      {숫자} : 숫자만큼 반복
      {숫자1, 숫자2} 숫자1 이상 숫자2 이하만큼 반복
      {숫자,} 숫자 이상만큼 반복
      [] : 대괄호 안의 문자중 한 개의 문자와 매치 
        ex) [amk] a or m or k 중 하나라도 존재하면 매칭 되는 것으로 간주
            [a-zA-Z] 알파벳 전체 범위
      [^문자] 해당 문자를 제외한 문자 매치
      |   A|B : A 또는 B의 의미
      
      | : 역 슬래쉬 문자 자체 의미
      |d : 모든 숫자를 의미  [0-9]와 같음
      |D : 숫자를 제외한 모든 문자 의미 [^0-9]와 같음
      |s : 공백을 의미 [|t|n|r|f|v]와 같음
      |S : 공백을 제외한 문자 의미 [^|t|n|r|f|v]와 같음
      |w : 문자 또는 숫자를 의미. [a-zA-Z0-9]와 같음
      |W : 문자 또는 숫자가 아닌 문자 [^a-zA-Z0-9]와 같음
      
      
      re.compile() : 정규표현식을 컴파일하는 함수.
      re.search() : 문자열 전체에 대해 정규표현식과 매치되는지 검색
      re.match() : 문자열의 처음이 정규표현식과 매치되는지 검색
      re.split() : 정규 표현식을 기준으로 문자열을 분리하여 리스트로 리턴
      re.findall() : 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴. 매치되는 문자열 없으면 빈 리스트가 리턴
      re..finditer() : 문자열에서 정규 표현식과 매치되느 ㄴ모든 경우의 문자열에 대한 이터레이터 객체를 리턴
      re.sub() : 문자열에서 정규표현식과 일치하는 부분에 대해 다른 문자열로 대체
      
      
      re.match() and re.search()
      search()는 전체에 대해 문자열이 매치하는지 확인, match() 문자열의 첫 부분부터 정규 표현식과 매치되는지 확인
      
      re.split()
      re.split(" ", text) 공백을 기준으로 문자열 분리
      
      re.findall("\d+", text) 전체 텍스트에서 숫자만 찾아내서 리스트로 리턴
      
      re.sub()
      ex) re.sub('[^a-zA-Z]',' ',text) 특수 문자를 제거하고 공백으로 처리
      
      
6) 정수 인코딩
  - 각 단어를 정수에 맵핑
  1.정수 인코딩
    1) dictionary
        from nltk.tokenize import sent_tokenize
        from nltk.tokenize import word_tokenize
        from nltk.corpus import stopwords

        text = "Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders."
        text = sent_tokenize(text)
        vocab = {}
        sentences = []
        stop_words = set(stopwords.words("english"))

        for i in text:
            sentences = word_tokenize(i)
            result = []

            for word in sentences:
                word = word.lower()
                if word not in stop_words:
                    if len(word) > 2:
                        result.append(word)
                        if word not in vocab:
                            vocab[word] = 0
                        vocab[word] += 1
            sentences.append(result)
        print(sentences)
        ['Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders.']

        print(vocab)
        {'recommender': 1, 'systems': 1, 'used': 1, 'variety': 1, 'areas': 1, 'commonly': 1, 'recognised': 1, 'examples': 1, 'taking': 1, 'form': 1, 'playlist': 1, 'generators': 1, 'video': 1, 'music': 1, 'services': 1, 'product': 1, 'recommenders': 3, 'online': 1, 'stores': 1, 'content': 2, 'social': 1, 'media': 1, 'platforms': 1, 'open': 1, 'web': 1}    

        vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)
        print(vocab_sorted)
        [('recommenders', 3), ('content', 2), ('recommender', 1), ('systems', 1), ('used', 1), ('variety', 1), ('areas', 1), ('commonly', 1), ('recognised', 1), ('examples', 1), ('taking', 1), ('form', 1), ('playlist', 1), ('generators', 1), ('video', 1), ('music', 1), ('services', 1), ('product', 1), ('online', 1), ('stores', 1), ('social', 1), ('media', 1), ('platforms', 1), ('open', 1), ('web', 1)]



        word_to_index = {}
        index = 0
        for (word, frequency) in vocab_sorted:
            if frequency > 1:
                index = index + 1
                word_to_index[word] = index
        print(word_to_index)

        가장 빈도수가 높은 순으로 빈도수 1인 단어 전부 제외.

        vocab_size = 5
        words_frequency = [i for i,c in word_to_index.items() if c >= vocab_size + 1]
        for i in words_frequency:
            del word_to_index[i]
        print(word_to_index)

        {'recommenders': 1, 'content': 2}

        word_to_index 빈도수가 높은 상위 5개의 단어 저장.
        각 단어는 정수로 인코딩 됨. 단어 집합에 존재 하지 않는 단어들을 Out-of-vocabulary = 'OOV'


     2) Counter
      - words = np.hstack(sentences)
        print(words)
        ['Recommender' 'systems' 'are' 'used' 'in' 'a' 'variety' 'of' 'areas' ','
         'with' 'commonly' 'recognised' 'examples' 'taking' 'the' 'form' 'of'
         'playlist' 'generators' 'for' 'video' 'and' 'music' 'services' ','
         'product' 'recommenders' 'for' 'online' 'stores' ',' 'or' 'content'
         'recommenders' 'for' 'social' 'media' 'platforms' 'and' 'open' 'web'
         'content' 'recommenders' '.' 'recommender' 'systems' 'used' 'variety'
         'areas' 'commonly' 'recognised' 'examples' 'taking' 'form' 'playlist'
         'generators' 'video' 'music' 'services' 'product' 'recommenders' 'online'
         'stores' 'content' 'recommenders' 'social' 'media' 'platforms' 'open'
         'web' 'content' 'recommenders']

       vocab = Counter(words)
       print(vocab)

       Counter({'recommenders': 6, 'content': 4, ',': 3, 'for': 3, 'systems': 2, 'used': 2, 'variety': 2, 'of': 2, 'areas': 2, 'commonly': 2, 'recognised': 2, 'examples': 2, 'taking': 2, 'form': 2, 'playlist': 2, 'generators': 2, 'video': 2, 'and': 2, 'music': 2, 'services': 2, 'product': 2, 'online': 2, 'stores': 2, 'social': 2, 'media': 2, 'platforms': 2, 'open': 2, 'web': 2, 'Recommender': 1, 'are': 1, 'in': 1, 'a': 1, 'with': 1, 'the': 1, 'or': 1, '.': 1, 'recommender': 1})


     3) FreqDist
      from nltk import FreqDist
      import numpy as np

      vocab = FreqDist(np.hstack(sentences))
      print(vocab["recommenders"])
      6

      vocab = vocab.most_common(5) #등장 빈도수가 높은 상위 5개의 단어 저장

    4) enumerate
      enumerate()는 순서가 있는 자료형을 입력으로 받아 인덱스를 순차적으로 함께 리턴

      test = ['a','b','c','d','e']
      for index, value in enumerate(test):
      print("value : {}, index : {}".format(value, index))

      value : a, index : 0
      value : b, index : 1
      value : c, index : 2
      value : d, index : 3
      value : e, index : 4


  2. 케라스의 텍스트 전처리
  
     from tensorflow.keras.preprocessing.text import Tokenizer
     
     tokenizer = Tokenizer()
     tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합 생성
     print(tokenizer.word_index)
     {'recommenders': 1, 'content': 2, 'for': 3, 'recommender': 4, 'systems': 5, 'used': 6, 'variety': 7, 'of': 8, 'areas': 9, 'commonly': 10, 'recognised': 11, 'examples': 12, 'taking': 13, 'form': 14, 'playlist': 15, 'generators': 16, 'video': 17, 'and': 18, 'music': 19, 'services': 20, 'product': 21, 'online': 22, 'stores': 23, 'social': 24, 'media': 25, 'platforms': 26, 'open': 27, 'web': 28, 'are': 29, 'in': 30, 'a': 31, 'with': 32, 'the': 33, 'or': 34}
     print(tokenizer.word_counts)
     OrderedDict([('recommender', 2), ('systems', 2), ('are', 1), ('used', 2), ('in', 1), ('a', 1), ('variety', 2), ('of', 2), ('areas', 2), ('with', 1), ('commonly', 2), ('recognised', 2), ('examples', 2), ('taking', 2), ('the', 1), ('form', 2), ('playlist', 2), ('generators', 2), ('for', 3), ('video', 2), ('and', 2), ('music', 2), ('services', 2), ('product', 2), ('recommenders', 6), ('online', 2), ('stores', 2), ('or', 1), ('content', 4), ('social', 2), ('media', 2), ('platforms', 2), ('open', 2), ('web', 2)])
     print(tokenizer.texts_to_sequences(sentences)) #입력으로 들어온 코퍼스에 대해 정해진 인덱스로 변환
     [[4], [5], [29], [6], [30], [31], [7], [8], [9], [], [32], [10], [11], [12], [13], [33], [14], [8], [15], [16], [3], [17], [18], [19], [20], [], [21], [1], [3], [22], [23], [], [34], [2], [1], [3], [24], [25], [26], [18], [27], [28], [2], [1], [], [4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 1, 22, 23, 2, 1, 24, 25, 26, 27, 28, 2, 1]]
     
     tokenizer = Tokenizer(num_words = 숫자) 상위 수의 단어만 사용하겠다고 지정
     vocab_size = 5
     tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV') # 빈도수 상위 5개 단어 사용, 0과 OOV 고려해서 단어 집합 크기 + 2
     tokenizer.fit_on_texts(sentences)
     
     print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))
     
     케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 사용
     
7) 패딩
  - 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업
    
    from tensorflow.keras.preprocessing.texdt import Tokenizer()
    sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(sentences)  #빈도수를 기준으로 단어 집합 생성
    encoded = tokenizer.texts_to_sequences(sentences)
    print(encoded)
    [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]
    max_len = max(len(item)) for item in encoded)
    print(max_len) #7
    
    가장 길이가 긴 문장의 길이는 7. 모든 문장의 길이 7로 변환
    
    for item in encoded:
      while len(item) < max_len:
        item.append(0)
        
    padded_np = np.array(encoded)
    padded_np
    
    array([[ 1,  5,  0,  0,  0,  0,  0],
       [ 1,  8,  5,  0,  0,  0,  0],
       [ 1,  3,  5,  0,  0,  0,  0],
       [ 9,  2,  0,  0,  0,  0,  0],
       [ 2,  4,  3,  2,  0,  0,  0],
       [ 3,  2,  0,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  2,  0,  0,  0,  0],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0,  0,  0]])
       
       
     
    케라스 전처리 도구 패딩
      from tensorflow.keras.preprocessing.sequence import pad_sequences
      
      encoded = tokenizer.texts_to_sequences(sentences)
      print(encoded)
      [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]
      
      padded = pad_sequences(encoded)
      padded
      
      array([[ 0,  0,  0,  0,  0,  1,  5],
       [ 0,  0,  0,  0,  1,  8,  5],
       [ 0,  0,  0,  0,  1,  3,  5],
       [ 0,  0,  0,  0,  0,  9,  2],
       [ 0,  0,  0,  2,  4,  3,  2],
       [ 0,  0,  0,  0,  0,  3,  2],
       [ 0,  0,  0,  0,  1,  4,  6],
       [ 0,  0,  0,  0,  1,  4,  6],
       [ 0,  0,  0,  0,  1,  4,  2],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 0,  0,  0,  1, 12,  3, 13]])
       
       padded = pad_sequences(encoded, padding = 'post')
       
       array([[ 1,  5,  0,  0,  0,  0,  0],
       [ 1,  8,  5,  0,  0,  0,  0],
       [ 1,  3,  5,  0,  0,  0,  0],
       [ 9,  2,  0,  0,  0,  0,  0],
       [ 2,  4,  3,  2,  0,  0,  0],
       [ 3,  2,  0,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  2,  0,  0,  0,  0],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0,  0,  0]])
       
       (padded == padded_np).all()  #결과가 동일한지 비교
       
       padded = pad_sequences(encoded, padding = 'post', maxlen = 5)  # 길이 제한 길이가 5보다 짧으면 0으로 패딩 5보다 길면 데이터 손실
       
       last_value = len(tokenizer.word_index) + 1 #단어 집합의 크기보다 1 큰 숫자 이용
       print(last_value) #14  현재 단어가 총 13개


      padded =pad_sequences(encoded, padding = 'post', value = last_value)
      padded                     # 0 이 아닌 last_value로 패딩
     
      array([[ 1,  5, 14, 14, 14, 14, 14],
       [ 1,  8,  5, 14, 14, 14, 14],
       [ 1,  3,  5, 14, 14, 14, 14],
       [ 9,  2, 14, 14, 14, 14, 14],
       [ 2,  4,  3,  2, 14, 14, 14],
       [ 3,  2, 14, 14, 14, 14, 14],
       [ 1,  4,  6, 14, 14, 14, 14],
       [ 1,  4,  6, 14, 14, 14, 14],
       [ 1,  4,  2, 14, 14, 14, 14],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13, 14, 14, 14]])
       
       
       
8) 원 핫 인코딩
  1. 원 핫 인코딩
    - 단어 집합의 크기를 벡터 차원으로 하고 , 표현하고 싶은 단어의 인덱스에 1의 값을 부여, 다른 인덱스에 0을 부여하는 벡터 표현 방식
     (원-핫 벡터)
     
     (1) 각 단어에 고유한 인덱스 부여 (정수 인코딩)
     (2) 표현하고 싶은 단어의 인덱스의 위치에 1을 부여하고, 다른 단어 인덱스의 위치에는 0을 부여
     
   2. 케라스 원 핫 인코딩
    - from tensorflow.keras.preprocessing.text import Tokenizer
      from tensorflow.keras.utils import to_categorical
      
      text = "나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 햄버거 최고야"
      
      t = Tokenizer()
      t.fit_on_texts([text])
      print(t.word_index)
      {'점심': 1, '갈래': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}
      
      sub_text = "점심 먹으러 갈래 메뉴는 햄버거 최고야"
      encoded = t.texts_to_sequences([sub_text])[0]
      print(encoded) #[2, 5, 1, 6, 3, 7]
      
      one_hot = to_categorical(encoded)  #원 핫 인코딩 수행
      print(one_hot)
      
   3. 원 핫 인코딩의 한계
      - 단어의 개수가 늘어날 수록, 벡터를 저장하기 위한 필요 공간이 증가하는 단점이 있다. 또한 단어의 유사도 표현하지 못함
      
   
   
9) 데이터의 분리
  
  1. 지도 학습 (supervised Learning)
    - 지도 학습의 훈련 데이터는 정답이 적혀있는 레이블 데이터와 문제 데이터 같이 구성되어 있다.
    
  2. x와 y분리하기
    1) zip 함수 이용하여 분리
       - zip()함수는 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할
       X,y = zip(['a', 1], ['b', 2], ['c', 3])
       print(X) # ('a','b','c')
       print(y) # (1 ,2 ,3)
     
    2) 데이터프레임을 이용하여 분리
      import pandas as pd
      
      values = [['당신에게 드리는 마지막 혜택!', 1],
      ['내일 뵐 수 있을지 확인 부탁드...', 0],
      ['도연씨. 잘 지내시죠? 오랜만입...', 0],
      ['(광고) AI로 주가를 예측할 수 있다!', 1]]
      columns = ['메일 본문', '스팸 메일 유무']
      
      df = pd.DataFrame(values, columns = columns)
      df
      
      
      	          메일 본문	스팸 메일 유무
        0	      당신에게 드리는 마지막 혜택!	1
        1	  내일 뵐 수 있을지 확인 부탁드...	0
        2	  도연씨. 잘 지내시죠? 오랜만입...	0
        3	(광고) AI로 주가를 예측할 수 있다!	1
     
     
        X = df['메일 본문']
        y = df['스팸 메일 유무']
        
    3) Numpy를 이용하여 분리
      array = np.arange(0,16).reshape((4,4))
      print(array)
     
     
       [[ 0  1  2  3]
       [ 4  5  6  7]
       [ 8  9 10 11]
       [12 13 14 15]]

       X = array[:, :3]
       print(X)

       [[ 0  1  2]
       [ 4  5  6]
       [ 8  9 10]
       [12 13 14]]

       y = array[:,3]
       [3 7 11 15]
   3. 테스트 데이터 분리하기
    
      1) 사이킷 런을 이용하여 분리
        from sklearn.model_selection import train_test_split
        x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 1234)
        
        x, y = np.arange(10).reshape((5,2)), range(5)
        
        x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.33, random_state = 1234) # 3분의 1만 test데이터로 지정
     
      2) 수동으로 분리
          x, y = np.arange(0,24).reshape((12,2)), range(12)
          number_of_train = int(len(x) * 0.8)
          number_of_test = int(len(x) - number_of_train)

          x_test = x[number_of_train:]
          y_test = y[number_of_train:]
          x_train = x[:number_of_train]
          y_train = y[:number_of_train]
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
